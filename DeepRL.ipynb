{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yroSYUnGNfdh"
      },
      "source": [
        "# Deep Reinforcement Learning\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvs0fyVcTssp"
      },
      "source": [
        " Reinforcement Learning (RL) is an approach wherein an agent learns to make sequential decisions by interacting with an environment. The objective is for the agent to maximize the cumulative reward it receives over time.\n",
        " The agent goes through this process by repeatedly evaluating the consequences of its actions, trying to select actions that lead to better outcomes.\n",
        "\n",
        "To do this, we will use Gym, an platform for developing and comparing reinforcement learning algorithms. Gym provides an interface for interacting with different environments, it accepts actions from agents and plays them out in an environment, providing rewards.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14HoqbdQNdoi"
      },
      "source": [
        "## Environment\n",
        "\n",
        "We will be using `CartPole` environment from gym's library for this assignment.  In this environment, a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
        "\n",
        "You can use the code below to run an instance of a random agent in this environment and see the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDUl4VKUTI6a"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "def show_video(path):\n",
        "    mp4 = open(path, 'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    return HTML(\"\"\"\n",
        "    <video width=400 controls>\n",
        "          <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgJvrYtuOJ6i",
        "outputId": "07f699ee-8a96-4216-ff42-1f4068f959c3"
      },
      "outputs": [],
      "source": [
        "!pip install gym[atari,accept-rom-license] -qq\n",
        "!pip install imageio -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAaKrhvfKe25"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import gym\n",
        "import imageio\n",
        "import numpy as np\n",
        "from gym import spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH0xWnEWToMa"
      },
      "source": [
        "We use `gym.make()` to make an instance of a certain environemtn. We can then use `.step()` method which accepts an action as input and performs it. Before that we reset the environment to its initial state by using `.reset()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RlavojcVATC",
        "outputId": "08c5664a-c624-4546-8e74-aded27c31fe1"
      },
      "outputs": [],
      "source": [
        "env_name = 'CartPole-v1'\n",
        "\n",
        "# Create an instance of the environment\n",
        "env = gym.make(env_name)\n",
        "\n",
        "env.reset()\n",
        "\n",
        "frames = []\n",
        "\n",
        "for _ in range(500):\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "\n",
        "    # render this frame and add to the list of frames\n",
        "    render_output = env.render(mode='rgb_array') # Change render mode to 'rgb_array'\n",
        "    if render_output is not None:\n",
        "        frames.append(render_output)\n",
        "\n",
        "    if done:\n",
        "        env.reset()\n",
        "\n",
        "env.close()\n",
        "imageio.mimsave('./cartpole.mp4', frames, fps=25) # Save the video file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "LTW3cWeXTbAB",
        "outputId": "48677fbd-3880-42dc-e1fd-62121d514f03"
      },
      "outputs": [],
      "source": [
        "show_video('./cartpole.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRqBG6pnZE36"
      },
      "source": [
        "As you can see, the cart fails to keep the balance of the pole. In the next section we will train an agent to learn how to perform this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXVxCrWQqg-8"
      },
      "source": [
        "## Algorithm\n",
        "We will be using A2C algorithm.\n",
        "\n",
        "Advantage Actor-Critic (A2C) is a reinforcement learning algorithm.\n",
        "It consists of an actor (which predicts the best action based on the current state) and a critic (which estimates the state's value function to measure expected future rewards).\n",
        "\n",
        "We will implement this together step by step.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DkxgycMx2gy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "from collections import deque\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDk5mnV8o4r9"
      },
      "source": [
        "## Neural Network\n",
        "\n",
        "Here we design a simple feed forward model to embed the observation from the environment to a hidden layer. We then use two fully connected layers on top of the hidden layer, to predict the next action and estimate the value of current state. This acts as both actor, and critic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_xktF4MxvFT"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_outputs):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) # Fully connected layer 1\n",
        "        self.fc_actor = nn.Linear(hidden_size, num_outputs) # Actor layer\n",
        "        self.fc_critic = nn.Linear(hidden_size, 1) # Critic layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x)) # Apply ReLU activation to the hidden layer\n",
        "        action_probs = F.softmax(self.fc_actor(x), dim=-1) # Softmax to get action probabilities\n",
        "        value = self.fc_critic(x) # Output value estimation for the state\n",
        "        return action_probs, value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmFYH_ls08da"
      },
      "source": [
        "## A2C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mq8eMC3Z4gM"
      },
      "source": [
        "The A2C algorithm aims to jointly train both the actor and the critic to improve the policy. It does this by updating the parameters\n",
        "of the actor to increase the likelihood of good actions and updating the parameters\n",
        "of the critic to better estimate the value function.\n",
        "\n",
        "In each iteration A2C plays the until it ends. During this time it records log probabality of actions, rewards, and predicted values in each step. These values will be used to update the model at the end of this trajectory.\n",
        "\n",
        "The actor is updated using the objective below:\n",
        "\n",
        "$$ L_{\\text{actor}} = -\\log \\pi(a|s;\\theta) \\times A(s, a) $$\n",
        "Where advantage is calculated as:\n",
        "$$A(s, a) = Q(s, a) - V(s) $$\n",
        "\n",
        "Namely the function $Q(s,a)$ is the estimated value of taking action\n",
        "$a$\n",
        " in state\n",
        "$s$.\n",
        "$V(s)$ is the predicted value of our critic.\n",
        "\n",
        "This loss function aims to improve the probability of playing actions that result in higher rewards.\n",
        "\n",
        "As for the critic the loss function is defined as a simple mean square loss between actual value of an state and the predicted one:\n",
        "\n",
        "$$ L_{\\text{critic}} = \\frac{1}{2} ( R - V(s))^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3YES4iP1DYb"
      },
      "outputs": [],
      "source": [
        "class A2CAgent:\n",
        "    def __init__(self, env, num_episodes=1000, max_steps=500, gamma=0.99, lr=1e-3, hidden_size=256):\n",
        "        self.env = env\n",
        "        self.num_episodes = num_episodes\n",
        "        self.max_steps = max_steps\n",
        "        self.gamma = gamma\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Get the size of the observation space and the number of actions\n",
        "        input_size = env.observation_space.shape[0]\n",
        "        num_outputs = env.action_space.n\n",
        "\n",
        "        # Define your actor-critic network\n",
        "        self.policy_net = ActorCritic(input_size, hidden_size, num_outputs).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.critic_loss = nn.MSELoss()\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        action_probs, _ = self.policy_net(state)\n",
        "        action = torch.multinomial(action_probs, 1).item()\n",
        "        return action\n",
        "\n",
        "    def compute_returns(self, rewards):\n",
        "        returns = []\n",
        "        R = 0\n",
        "        for r in reversed(rewards):\n",
        "            R = r + self.gamma * R\n",
        "            returns.insert(0, R)\n",
        "        returns = torch.tensor(returns).to(self.device)\n",
        "        return returns\n",
        "\n",
        "    def train(self):\n",
        "        episode_rewards = []\n",
        "\n",
        "        for episode in tqdm(range(self.num_episodes)):\n",
        "            state = self.env.reset()\n",
        "            if state is None or len(state) != self.env.observation_space.shape[0]:\n",
        "                print(f\"Invalid initial state: {state}\")\n",
        "                continue\n",
        "\n",
        "            log_probs = []\n",
        "            values = []\n",
        "            rewards = []\n",
        "            episode_reward = 0\n",
        "\n",
        "            for step in range(self.max_steps):\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "                if next_state is None or len(next_state) != self.env.observation_space.shape[0]:\n",
        "                    print(f\"Invalid next state at step {step}: {next_state}\")\n",
        "                    break\n",
        "\n",
        "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "                _, value = self.policy_net(state_tensor)\n",
        "                log_prob = torch.log(self.policy_net(state_tensor)[0][0][action])\n",
        "\n",
        "                log_probs.append(log_prob)\n",
        "                values.append(value)\n",
        "                rewards.append(reward)\n",
        "                episode_reward += reward\n",
        "\n",
        "                state = next_state\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            episode_rewards.append(episode_reward)\n",
        "\n",
        "            # Calculate the discounted rewards\n",
        "            returns = self.compute_returns(rewards)\n",
        "\n",
        "            # Convert lists to tensors\n",
        "            log_probs = torch.stack(log_probs)\n",
        "            values = torch.stack(values).squeeze()\n",
        "\n",
        "            # Calculate advantage\n",
        "            advantage = returns - values\n",
        "\n",
        "            # Compute actor and critic loss\n",
        "            actor_loss = -(log_probs * advantage.detach()).mean()\n",
        "            critic_loss = self.critic_loss(values, returns)\n",
        "            loss = actor_loss + critic_loss\n",
        "\n",
        "            # Perform backpropagation\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            print(f\"Episode {episode + 1}, Reward: {episode_reward}\")\n",
        "\n",
        "        self.env.close()\n",
        "        return episode_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb1bgYVWeCdD"
      },
      "source": [
        "Define the model and set hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngtRu4PH4I0L",
        "outputId": "5f5fdd96-5ccc-4ff2-8947-a0c51b99b7c6"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "num_episodes = 1000\n",
        "max_steps = 500\n",
        "lr = 1e-3\n",
        "hidden_size = 256\n",
        "\n",
        "a2c_model = A2CAgent(env, num_episodes=num_episodes, max_steps=max_steps, lr=lr, hidden_size=hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0SN8RoJeFZy"
      },
      "source": [
        "Train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s6WdYRz4V4z",
        "outputId": "2a0c518b-9e06-4c60-8bc1-c763dbe7f2df"
      },
      "outputs": [],
      "source": [
        "rewards = a2c_model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMViOQSeyvqx"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GmFp1D9eG1S"
      },
      "source": [
        "Use the `choose_action` method of the trained agent to evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i75PdIqbI0PB",
        "outputId": "ac359756-66b7-4109-8aef-bfa443983f52"
      },
      "outputs": [],
      "source": [
        "env_name = 'CartPole-v1'\n",
        "env = gym.make(env_name, render_mode='rgb_array')\n",
        "\n",
        "num_episodes = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming a2c_model is already trained and available\n",
        "model = a2c_model\n",
        "\n",
        "frames = []\n",
        "episode_rewards = []\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    if isinstance(state, tuple):  # Handle different Gym API versions\n",
        "        state = state[0]\n",
        "\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Add the current frame to the list if it's the first episode\n",
        "        if i == 0:\n",
        "            frame = env.render()\n",
        "\n",
        "            # Check if frame is a list and handle accordingly\n",
        "            if isinstance(frame, list):\n",
        "                frame = np.array(frame[0])  # Assuming the first element of the list is the frame\n",
        "\n",
        "            # Check if the frame has the expected number of channels\n",
        "            if frame.shape[-1] not in (1, 2, 3, 4):\n",
        "                # Convert the frame to RGB if it has a different number of channels\n",
        "                frame = frame[..., :3]  # Assuming the first 3 channels are RGB\n",
        "            frames.append(frame)\n",
        "\n",
        "        # Debugging information\n",
        "        print(f\"State shape: {state.shape}, State type: {type(state)}\")\n",
        "\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            action_probs, _ = model.policy_net(state)\n",
        "        action = torch.multinomial(action_probs, 1).item()\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        if isinstance(next_state, tuple):  # Handle different Gym API versions\n",
        "            next_state = next_state[0]\n",
        "\n",
        "        episode_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "    episode_rewards.append(episode_reward)\n",
        "    print(f\"Episode {i + 1} Reward: {episode_reward}\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "episode_rewards = np.array(episode_rewards)\n",
        "print(f\"Average Reward over {num_episodes} episodes: {np.mean(episode_rewards)}\")\n",
        "imageio.mimsave('./test.mp4', frames, fps=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "85Uys0DYLmwB",
        "outputId": "7acfbdd5-88a9-4bf4-ea81-41fde0a947b2"
      },
      "outputs": [],
      "source": [
        "show_video('./test.mp4')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
